{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Install necesessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T08:08:27.424103Z",
     "iopub.status.busy": "2022-02-02T08:08:27.423689Z",
     "iopub.status.idle": "2022-02-02T08:09:49.856299Z",
     "shell.execute_reply": "2022-02-02T08:09:49.855110Z",
     "shell.execute_reply.started": "2022-02-02T08:08:27.423995Z"
    }
   },
   "outputs": [],
   "source": [
    "# # norfair dependencies\n",
    "# %cd /kaggle/input/norfair031py3/\n",
    "# !pip install commonmark-0.9.1-py2.py3-none-any.whl -f ./ --no-index\n",
    "# !pip install rich-9.13.0-py3-none-any.whl\n",
    "\n",
    "# !mkdir /kaggle/working/tmp\n",
    "# !cp -r /kaggle/input/norfair031py3/filterpy-1.4.5/filterpy-1.4.5/ /kaggle/working/tmp/\n",
    "# %cd /kaggle/working/tmp/filterpy-1.4.5/\n",
    "# !pip install .\n",
    "# !rm -rf /kaggle/working/tmp\n",
    "\n",
    "# # norfair\n",
    "# %cd /kaggle/input/norfair031py3/\n",
    "# !pip install norfair-0.3.1-py3-none-any.whl -f ./ --no-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T08:09:49.859197Z",
     "iopub.status.busy": "2022-02-02T08:09:49.858899Z",
     "iopub.status.idle": "2022-02-02T08:09:52.410774Z",
     "shell.execute_reply": "2022-02-02T08:09:52.409527Z",
     "shell.execute_reply.started": "2022-02-02T08:09:49.859163Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import ast\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "import torch\n",
    "import sys\n",
    "from PIL import Image as Img\n",
    "from IPython.display import display\n",
    "from norfair import Detection, Tracker\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "sys.path.append('tensorflow-great-barrier-reef')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Set model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T08:09:52.412634Z",
     "iopub.status.busy": "2022-02-02T08:09:52.412387Z",
     "iopub.status.idle": "2022-02-02T08:09:53.976959Z",
     "shell.execute_reply": "2022-02-02T08:09:53.975775Z",
     "shell.execute_reply.started": "2022-02-02T08:09:52.412602Z"
    }
   },
   "outputs": [],
   "source": [
    "FOLD = 1\n",
    "\n",
    "ROOT_DIR  = 'tensorflow-great-barrier-reef'\n",
    "DATASET_PATH = 'tensorflow-great-barrier-reef/train_images/'\n",
    "\n",
    "# !mkdir -p /root/.config/Ultralytics\n",
    "# !cp /kaggle/input/yolov5-font/Arial.ttf /root/.config/Ultralytics/\n",
    "\n",
    "path_m6 = 'yolov5-models/yolov5m-1920.pt'\n",
    "path_l6 = 'yolov5-models/yolov5l_1920.pt'\n",
    "path_s6_3840 = 'yolov5-models/yolov5_3840_f1.pt'\n",
    "path_s6_3000_20_pct_f1 = 'yolov5-models/yolov5s6_3000_20_pcnt_empty_images_f1.pt'\n",
    "yolov5s6_1920_batch_8 = 'yolov5s6_1920_batch_8/f0.pt'\n",
    "yolov5s6_1920_batch_8_groupk = f'yolov5s6_1920_batch_8_groupk/f{FOLD}.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluation\n",
    "\n",
    "## Evaluation utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T08:09:53.981580Z",
     "iopub.status.busy": "2022-02-02T08:09:53.981182Z",
     "iopub.status.idle": "2022-02-02T08:09:54.010464Z",
     "shell.execute_reply": "2022-02-02T08:09:54.009244Z",
     "shell.execute_reply.started": "2022-02-02T08:09:53.981527Z"
    }
   },
   "outputs": [],
   "source": [
    "def calc_iou(bboxes1, bboxes2, bbox_mode='xywh'):\n",
    "    assert len(bboxes1.shape) == 2 and bboxes1.shape[1] == 4\n",
    "    assert len(bboxes2.shape) == 2 and bboxes2.shape[1] == 4\n",
    "    \n",
    "    bboxes1 = bboxes1.copy()\n",
    "    bboxes2 = bboxes2.copy()\n",
    "    \n",
    "    if bbox_mode == 'xywh':\n",
    "        bboxes1[:, 2:] += bboxes1[:, :2]\n",
    "        bboxes2[:, 2:] += bboxes2[:, :2]\n",
    "\n",
    "    x11, y11, x12, y12 = np.split(bboxes1, 4, axis=1)\n",
    "    x21, y21, x22, y22 = np.split(bboxes2, 4, axis=1)\n",
    "    xA = np.maximum(x11, np.transpose(x21))\n",
    "    yA = np.maximum(y11, np.transpose(y21))\n",
    "    xB = np.minimum(x12, np.transpose(x22))\n",
    "    yB = np.minimum(y12, np.transpose(y22))\n",
    "    interArea = np.maximum((xB - xA + 1), 0) * np.maximum((yB - yA + 1), 0)\n",
    "    boxAArea = (x12 - x11 + 1) * (y12 - y11 + 1)\n",
    "    boxBArea = (x22 - x21 + 1) * (y22 - y21 + 1)\n",
    "    iou = interArea / (boxAArea + np.transpose(boxBArea) - interArea)\n",
    "    return iou\n",
    "\n",
    "def f_beta(tp, fp, fn, beta=2):\n",
    "    return (1+beta**2)*tp / ((1+beta**2)*tp+beta**2*fn+fp)\n",
    "\n",
    "def calc_is_correct_at_iou_th(gt_bboxes, pred_bboxes, iou_th, verbose=False):\n",
    "    gt_bboxes = gt_bboxes.copy()\n",
    "    pred_bboxes = pred_bboxes.copy()\n",
    "    \n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    for k, pred_bbox in enumerate(pred_bboxes): # fixed in ver.7\n",
    "        ious = calc_iou(gt_bboxes, pred_bbox[None, 1:])\n",
    "        max_iou = ious.max()\n",
    "        if max_iou > iou_th:\n",
    "            tp += 1\n",
    "            gt_bboxes = np.delete(gt_bboxes, ious.argmax(), axis=0)\n",
    "        else:\n",
    "            fp += 1\n",
    "        if len(gt_bboxes) == 0:\n",
    "            fp += len(pred_bboxes) - (k + 1) # fix in ver.7\n",
    "            break\n",
    "\n",
    "    fn = len(gt_bboxes)\n",
    "    return tp, fp, fn\n",
    "\n",
    "def calc_is_correct(gt_bboxes, pred_bboxes):\n",
    "    \"\"\"\n",
    "    gt_bboxes: (N, 4) np.array in xywh format\n",
    "    pred_bboxes: (N, 5) np.array in conf+xywh format\n",
    "    \"\"\"\n",
    "    if len(gt_bboxes) == 0 and len(pred_bboxes) == 0:\n",
    "        tps, fps, fns = 0, 0, 0\n",
    "        return tps, fps, fns\n",
    "    \n",
    "    elif len(gt_bboxes) == 0:\n",
    "        tps, fps, fns = 0, len(pred_bboxes)*11, 0\n",
    "        return tps, fps, fns\n",
    "    \n",
    "    elif len(pred_bboxes) == 0:\n",
    "        tps, fps, fns = 0, 0, len(gt_bboxes)*11\n",
    "        return tps, fps, fns\n",
    "    \n",
    "    pred_bboxes = pred_bboxes[pred_bboxes[:,0].argsort()[::-1]] # sort by conf\n",
    "    \n",
    "    tps, fps, fns = 0, 0, 0\n",
    "    for iou_th in np.arange(0.3, 0.85, 0.05):\n",
    "        tp, fp, fn = calc_is_correct_at_iou_th(gt_bboxes, pred_bboxes, iou_th)\n",
    "        tps += tp\n",
    "        fps += fp\n",
    "        fns += fn\n",
    "    return tps, fps, fns\n",
    "\n",
    "def calc_f2_score(gt_bboxes_list, pred_bboxes_list, verbose=False):\n",
    "    \"\"\"\n",
    "    gt_bboxes_list: list of (N, 4) np.array in xywh format\n",
    "    pred_bboxes_list: list of (N, 5) np.array in conf+xywh format\n",
    "    \"\"\"\n",
    "    tps, fps, fns = 0, 0, 0\n",
    "    for gt_bboxes, pred_bboxes in zip(gt_bboxes_list, pred_bboxes_list):\n",
    "        tp, fp, fn = calc_is_correct(gt_bboxes, pred_bboxes)\n",
    "        tps += tp\n",
    "        fps += fp\n",
    "        fns += fn\n",
    "        if verbose:\n",
    "            num_gt = len(gt_bboxes)\n",
    "            num_pred = len(pred_bboxes)\n",
    "            print(f'num_gt:{num_gt:<3} num_pred:{num_pred:<3} tp:{tp:<3} fp:{fp:<3} fn:{fn:<3}')\n",
    "    return f_beta(tps, fps, fns, beta=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T08:09:54.013155Z",
     "iopub.status.busy": "2022-02-02T08:09:54.012011Z",
     "iopub.status.idle": "2022-02-02T08:09:54.028769Z",
     "shell.execute_reply": "2022-02-02T08:09:54.027970Z",
     "shell.execute_reply.started": "2022-02-02T08:09:54.013116Z"
    }
   },
   "outputs": [],
   "source": [
    "from norfair import Detection, Tracker\n",
    "\n",
    "# Helper to convert bbox in format [x_min, y_min, x_max, y_max, score] to norfair.Detection class\n",
    "def to_norfair(detects, frame_id):\n",
    "    result = []\n",
    "    for x_min, y_min, x_max, y_max, score in detects:\n",
    "        xc, yc = (x_min + x_max) / 2, (y_min + y_max) / 2\n",
    "        w, h = x_max - x_min, y_max - y_min\n",
    "        result.append(Detection(points=np.array([xc, yc]), scores=np.array([score]), data=np.array([w, h, frame_id])))\n",
    "        \n",
    "    return result\n",
    "\n",
    "# Euclidean distance function to match detections on this frame with tracked_objects from previous frames\n",
    "def euclidean_distance(detection, tracked_object):\n",
    "    return np.linalg.norm(detection.points - tracked_object.estimate)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T08:09:54.031095Z",
     "iopub.status.busy": "2022-02-02T08:09:54.030151Z",
     "iopub.status.idle": "2022-02-02T08:09:54.047426Z",
     "shell.execute_reply": "2022-02-02T08:09:54.046243Z",
     "shell.execute_reply.started": "2022-02-02T08:09:54.031051Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_prediction(img, bboxes, gts, show=True):\n",
    "    colors = [(0, 0, 255)]\n",
    "\n",
    "    obj_names = [\"s\"]\n",
    "\n",
    "    for box in bboxes:\n",
    "#         cv2.rectangle(img, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), (255,0,0), 2)\n",
    "        cv2.rectangle(img, (int(box[1]), int(box[2])), (int(box[1] + box[3]), int(box[2] + box[4])), (255,0,0), 2)\n",
    "        cv2.putText(img, f'{box[0]}', (int(box[1]), int(box[2])-3), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,0,0), 1, cv2.LINE_AA)\n",
    "        \n",
    "    for gt in gts:\n",
    "        cv2.rectangle(img, (int(gt[0]), int(gt[1])), (int(gt[0]+gt[2]), int(gt[1]+gt[3])), (0,255,0), 2)\n",
    "    \n",
    "    if show:\n",
    "        img = Img.fromarray(img).resize((960, 540))\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get GT bboxes from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T08:09:54.049824Z",
     "iopub.status.busy": "2022-02-02T08:09:54.049456Z",
     "iopub.status.idle": "2022-02-02T08:10:12.763284Z",
     "shell.execute_reply": "2022-02-02T08:10:12.762194Z",
     "shell.execute_reply.started": "2022-02-02T08:09:54.049780Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc48f488f97740bd9d701e8fe34ec53f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23501 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "718f819c3ca94f94b27c87aa9e5d0044",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23501 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca432d3a825048e098482a8710cd7ce9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23501 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No BBox: 79.07% | With BBox: 20.93%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1306ced01cdb492485f7e1e6bb0ca2d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23501 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dir = f'{DATASET_PATH}'\n",
    "imgs = [dir + f for f in ('video_2/5748.jpg',\n",
    "                          'video_2/5772.jpg',\n",
    "                          'video_2/5820.jpg',\n",
    "                          'video_1/4159.jpg', \n",
    "                          'video_1/4183.jpg', \n",
    "                          'video_1/4501.jpg', \n",
    "                          'video_1/5375.jpg', \n",
    "                          'video_1/5414.jpg',\n",
    "                          'video_1/5495.jpg',\n",
    "                          'video_1/4775.jpg', \n",
    "                          'video_0/9794.jpg', \n",
    "                          'video_0/4502.jpg', \n",
    "                          'video_0/9651.jpg', \n",
    "                          'video_0/9700.jpg',  \n",
    "                          'video_0/9674.jpg',\n",
    "                          'video_0/20.jpg', \n",
    "                          'video_0/17.jpg', \n",
    "                          'video_1/5474.jpg', \n",
    "                          'video_0/0.jpg')]\n",
    "\n",
    "def get_path(row):\n",
    "    row['image_path'] = f'{ROOT_DIR}/train_images/video_{row.video_id}/{row.video_frame}.jpg'\n",
    "    return row\n",
    "\n",
    "def get_bbox(annots):\n",
    "    bboxes = [list(annot.values()) for annot in annots]\n",
    "    return bboxes\n",
    "\n",
    "# Train Data\n",
    "df = pd.read_csv(f'{ROOT_DIR}/train.csv')\n",
    "# df = df[df.video_id == FOLD]\n",
    "df = df.progress_apply(get_path, axis=1)\n",
    "df['annotations'] = df['annotations'].progress_apply(lambda x: ast.literal_eval(x))\n",
    "df.head(2)\n",
    "\n",
    "df['num_bbox'] = df['annotations'].progress_apply(lambda x: len(x))\n",
    "data = (df.num_bbox>0).value_counts(normalize=True)*100\n",
    "print(f\"No BBox: {data[0]:0.2f}% | With BBox: {data[1]:0.2f}%\")\n",
    "\n",
    "df['bboxes'] = df.annotations.progress_apply(get_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_bbox</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>271</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      num_bbox\n",
       "fold          \n",
       "0         1100\n",
       "1          704\n",
       "2          654\n",
       "3          577\n",
       "4          564\n",
       "5          285\n",
       "6          238\n",
       "7          252\n",
       "8          274\n",
       "9          271"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FOLD_NUM = 10\n",
    "\n",
    "train = df[df.num_bbox>0]\n",
    "kf = GroupKFold(n_splits = FOLD_NUM) \n",
    "train = train.reset_index(drop=True)\n",
    "train['fold'] = -1\n",
    "for f, (train_idx, val_idx) in enumerate(kf.split(train, y = train.video_id.tolist(), groups=train.sequence)):\n",
    "    train.loc[val_idx, 'fold'] = f\n",
    "\n",
    "train.groupby('fold').agg({'num_bbox': 'count'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model\n",
    "\n",
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T08:10:12.765435Z",
     "iopub.status.busy": "2022-02-02T08:10:12.765076Z",
     "iopub.status.idle": "2022-02-02T08:10:12.796357Z",
     "shell.execute_reply": "2022-02-02T08:10:12.794860Z",
     "shell.execute_reply.started": "2022-02-02T08:10:12.765397Z"
    }
   },
   "outputs": [],
   "source": [
    "def clahe_hsv(img):\n",
    "    hsv_img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    h, s, v = hsv_img[:,:,0], hsv_img[:,:,1], hsv_img[:,:,2]\n",
    "    clahe = cv2.createCLAHE(clipLimit = 15.0, tileGridSize = (20,20))\n",
    "    v = clahe.apply(v)\n",
    "\n",
    "    hsv_img = np.dstack((h,s,v))\n",
    "\n",
    "    rgb = cv2.cvtColor(hsv_img, cv2.COLOR_HSV2RGB)\n",
    "    \n",
    "    return rgb\n",
    "\n",
    "def load_model(path, conf, iou):\n",
    "    model = torch.hub.load('yolov5', \n",
    "                          'custom', \n",
    "                          path = path,\n",
    "                          source='local',\n",
    "                          force_reload=True)  # local repo\n",
    "    model.conf = conf\n",
    "    model.iou = iou\n",
    "    model.classes = None   # (optional list) filter by class, i.e. = [0, 15, 16] for persons, cats and dogs\n",
    "    model.multi_label = False  # NMS multiple labels per box\n",
    "    model.max_det = 1000  # maximum number of detections per image\n",
    "    return model\n",
    "\n",
    "def evaluate(path, test_df, conf, iou, img_size, area_thr, visualize=False, augment=True, tracking=True):\n",
    "    # Tracker will update tracks based on detections from current frame\n",
    "    tracker = Tracker(\n",
    "        distance_function=euclidean_distance, \n",
    "        distance_threshold=30,\n",
    "        hit_inertia_min=3,\n",
    "        hit_inertia_max=6,\n",
    "        initialization_delay=1,\n",
    "    )\n",
    "        \n",
    "    # Save frame_id into detection to know which tracks have no detections on current frame\n",
    "    frame_id = 0\n",
    "    \n",
    "    model = load_model(path, conf, iou)\n",
    "    gt_bboxes_list, prd_bboxes_list = [], []\n",
    "\n",
    "    for idx, row in tqdm(test_df.iterrows()):\n",
    "        bboxes = np.empty((0,5), int)\n",
    "        gt_bboxes, pred_bboxes = [], []\n",
    "        \n",
    "        img_path = row.image_path\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "#         img = clahe_hsv(img)\n",
    "\n",
    "        # get GT bboxes for evaluation\n",
    "        for gt in row.bboxes:\n",
    "            gt_bbox = np.array(list(map(float, gt)))\n",
    "            gt_bboxes.append(gt_bbox)\n",
    "        gt_bboxes_list.append(np.array(gt_bboxes))\n",
    "\n",
    "        r = model(img, size=img_size, augment=augment)\n",
    "\n",
    "        if r.pandas().xyxy[0].shape[0] == 0:\n",
    "            anno = ''\n",
    "        else:\n",
    "            for idx, row in r.pandas().xyxy[0].iterrows():\n",
    "                  bboxes = np.append(bboxes, [[row.xmin, row.ymin, row.xmax, row.ymax, row.confidence]], axis=0)\n",
    "        \n",
    "        predictions = []\n",
    "        detects = []\n",
    "        \n",
    "        if len(bboxes) > 0:\n",
    "            for bbox in bboxes:\n",
    "                score = bbox[4]\n",
    "                width, height = int(bbox[2]-bbox[0]), int(bbox[3]-bbox[1])\n",
    "                area = width * height\n",
    "                \n",
    "                if area >= area_thr:\n",
    "                    detects.append([int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3]), score])\n",
    "                    predictions.append('{:.2f} {} {} {} {}'.format(score, int(bbox[0]), int(bbox[1]), width, height))\n",
    "                    pred_bboxes.append(np.array([score, int(bbox[0]), int(bbox[1]), width, height]))\n",
    "                    \n",
    "        #  Tracking\n",
    "        if tracking:\n",
    "            # Update tracks using detects from current frame\n",
    "            tracked_objects = tracker.update(detections=to_norfair(detects, frame_id))\n",
    "            for tobj in tracked_objects:\n",
    "                bbox_width, bbox_height, last_detected_frame_id = tobj.last_detection.data\n",
    "                if last_detected_frame_id == frame_id:  # Skip objects that were detected on current frame\n",
    "                    continue\n",
    "\n",
    "                # Add objects that have no detections on current frame to predictions\n",
    "                xc, yc = tobj.estimate[0]\n",
    "                x_min, y_min = int(round(xc - bbox_width / 2)), int(round(yc - bbox_height / 2))\n",
    "                score = tobj.last_detection.scores[0]\n",
    "                area = bbox_width * bbox_height\n",
    "                if area >= area_thr:\n",
    "                    predictions.append('{:.2f} {} {} {} {}'.format(score, x_min, y_min, bbox_width, bbox_height))\n",
    "                    pred_bboxes.append(np.array([score, x_min, y_min, bbox_width, bbox_height]))\n",
    "        \n",
    "        if visualize and idx < 3:\n",
    "            display(show_prediction(img, pred_bboxes, gt_bboxes))\n",
    "        \n",
    "        # get pred bboxes for evaluation\n",
    "        prd_bboxes_list.append(np.array(pred_bboxes))\n",
    "        \n",
    "        prediction_str = ' '.join(predictions)\n",
    "        \n",
    "        frame_id += 1\n",
    "\n",
    "    f2_score = calc_f2_score(gt_bboxes_list, prd_bboxes_list, verbose=False)\n",
    "\n",
    "    return f2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search optimal hypreparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T08:10:12.798897Z",
     "iopub.status.busy": "2022-02-02T08:10:12.798373Z",
     "iopub.status.idle": "2022-02-02T08:10:12.813938Z",
     "shell.execute_reply": "2022-02-02T08:10:12.812779Z",
     "shell.execute_reply.started": "2022-02-02T08:10:12.798844Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 ðŸš€ v6.0-224-g4c40933 torch 1.10.0+cu102 CUDA:0 (NVIDIA GeForce GTX 1070, 8116MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 280 layers, 12308200 parameters, 0 gradients, 16.2 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a54098dd715e4b81ab004b5e5f80c729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 ðŸš€ v6.0-224-g4c40933 torch 1.10.0+cu102 CUDA:0 (NVIDIA GeForce GTX 1070, 8116MiB)\n",
      "\n",
      "Fusing layers... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Image size is 1920, F2 is 0.6153908203597644\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Summary: 280 layers, 12308200 parameters, 0 gradients, 16.2 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63b23b9b3a3e41808b185385d4802517",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 ðŸš€ v6.0-224-g4c40933 torch 1.10.0+cu102 CUDA:0 (NVIDIA GeForce GTX 1070, 8116MiB)\n",
      "\n",
      "Fusing layers... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Image size is 2560, F2 is 0.6487859966120836\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Summary: 280 layers, 12308200 parameters, 0 gradients, 16.2 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4ffb5993b34456085970734af7128df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 ðŸš€ v6.0-224-g4c40933 torch 1.10.0+cu102 CUDA:0 (NVIDIA GeForce GTX 1070, 8116MiB)\n",
      "\n",
      "Fusing layers... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Image size is 3200, F2 is 0.6597956307258633\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Summary: 280 layers, 12308200 parameters, 0 gradients, 16.2 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1432b8e416f44d1bb1ddca9bb6a0763e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 ðŸš€ v6.0-224-g4c40933 torch 1.10.0+cu102 CUDA:0 (NVIDIA GeForce GTX 1070, 8116MiB)\n",
      "\n",
      "Fusing layers... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Image size is 3840, F2 is 0.6556507418726545\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Summary: 280 layers, 12308200 parameters, 0 gradients, 16.2 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2fd75aa72d94c2a8668aaf24be8d7ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "FOLD = 1\n",
    "sequences = train.loc[train.fold == FOLD, 'sequence'].unique()\n",
    "test_df = df[df.sequence.isin(sequences)]\n",
    "test_df.head()\n",
    "\n",
    "confthre = 0.2\n",
    "area_thr = 0\n",
    "img_size = 3200 # int(2000*3)\n",
    "conf = 0.28\n",
    "augment = True\n",
    "iou = 0.4\n",
    "\n",
    "# search for the best image size\n",
    "size_dict = {}\n",
    "best_size = 0\n",
    "best_f2 = 0\n",
    "\n",
    "for img_size in np.arange(1920, 7040, 640):\n",
    "    f2 = evaluate(yolov5s6_1920_batch_8_groupk, test_df, conf, iou, img_size, area_thr, augment=True, tracking=True, visualize=False)\n",
    "    size_dict[conf] = f2\n",
    "    print(80*'=')\n",
    "    print(f'Image size is {img_size}, F2 is {f2}')\n",
    "    print(80*'=')\n",
    "    if f2 > best_f2:\n",
    "        best_f2 = f2\n",
    "        best_size = img_size\n",
    "        \n",
    "print(f'Best image size is {best_size}, best F2 is {best_f2}')\n",
    "    \n",
    "# search for the best confidence threshold\n",
    "conf_dict = {}\n",
    "best_conf = 0\n",
    "best_f2 = 0\n",
    "\n",
    "for conf in np.arange(0.15, 0.65, 0.05):\n",
    "    f2 = evaluate(yolov5s6_1920_batch_8_groupk, test_df, conf, iou, best_size, area_thr, augment=True, tracking=True, visualize=False)\n",
    "    conf_dict[conf] = f2\n",
    "    print(80*'=')\n",
    "    print(f'Confidence threshold is {conf}, F2 is {f2}')\n",
    "    print(80*'=')\n",
    "    if f2 > best_f2:\n",
    "        best_f2 = f2\n",
    "        best_conf = conf\n",
    "        \n",
    "print(f'Best confidence threshold is {best_conf}, best F2 is {best_f2}')\n",
    "\n",
    "# search for the best IOU\n",
    "iou_dict = {}\n",
    "best_iou = 0\n",
    "best_f2 = 0\n",
    "\n",
    "for iou in np.arange(0.2, 0.65, 0.05):\n",
    "    f2 = evaluate(yolov5s6_1920_batch_8_groupk, test_df, best_conf, iou, best_size, area_thr, augment=True, tracking=True, visualize=False)\n",
    "    iou_dict[iou] = f2\n",
    "    print(80*'=')\n",
    "    print(f'IOU is {iou}, F2 is {f2}')\n",
    "    print(80*'=')\n",
    "    if f2 > best_f2:\n",
    "        best_f2 = f2\n",
    "        best_iou = iou\n",
    "\n",
    "print(f'Best IOU is {best_iou}, best F2 is {best_f2}')\n",
    "\n",
    "# search for the best area\n",
    "area_dict = {}\n",
    "best_area = 0\n",
    "best_f2 = 0\n",
    "\n",
    "for area_thr in np.arange(0, 450, 50):\n",
    "    f2 = evaluate(path_m6, test_df, best_conf, best_iou, best_size, area_thr, augment=True, tracking=True)\n",
    "    area_dict[area_thr] = f2\n",
    "    print(80*'=')\n",
    "    print(f'Area threshold is {area_thr}, F2 is {f2}')\n",
    "    print(80*'=')\n",
    "    if f2 > best_f2:\n",
    "        best_f2 = f2\n",
    "        best_area = area_thr\n",
    "\n",
    "print(f'Best area is {best_area}, best F2 is {best_f2}')\n",
    "\n",
    "\n",
    "# evaluate(yolov5s6_1920_batch_8_groupk, test_df, conf, iou, img_size, area_thr, \n",
    "#          visualize=False, augment=True, tracking=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Inference\n",
    "\n",
    "## Initialize environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T08:10:12.816988Z",
     "iopub.status.busy": "2022-02-02T08:10:12.816557Z",
     "iopub.status.idle": "2022-02-02T08:10:12.861731Z",
     "shell.execute_reply": "2022-02-02T08:10:12.860988Z",
     "shell.execute_reply.started": "2022-02-02T08:10:12.816951Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'greatbarrierreef.competition'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27060/1941073671.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgreatbarrierreef\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgreatbarrierreef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# initialize the environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0miter_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m# an iterator which loops over the test set and sample submission\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Kaggle/cots/tensorflow-great-barrier-reef/greatbarrierreef/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcompetition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_env\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'make_env'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'greatbarrierreef.competition'"
     ]
    }
   ],
   "source": [
    "import greatbarrierreef\n",
    "env = greatbarrierreef.make_env()# initialize the environment\n",
    "iter_test = env.iter_test()      # an iterator which loops over the test set and sample submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions with tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T08:10:12.863574Z",
     "iopub.status.busy": "2022-02-02T08:10:12.863004Z"
    }
   },
   "outputs": [],
   "source": [
    "submission_dict = {\n",
    "    'id': [],\n",
    "    'prediction_string': [],\n",
    "}\n",
    "\n",
    "model = load_model(yolov5s6_1920_batch_8_groupk_f0, conf, iou)\n",
    "\n",
    "#######################################################\n",
    "#                      Tracking                       #\n",
    "#######################################################\n",
    "\n",
    "# Tracker will update tracks based on detections from current frame\n",
    "# Matching based on euclidean distance between bbox centers of detections \n",
    "# from current frame and tracked_objects based on previous frames\n",
    "# You can check it's parameters in norfair docs\n",
    "# https://github.com/tryolabs/norfair/blob/master/docs/README.md\n",
    "tracker = Tracker(\n",
    "    distance_function=euclidean_distance, \n",
    "    distance_threshold=30,\n",
    "    hit_inertia_min=3,\n",
    "    hit_inertia_max=6,\n",
    "    initialization_delay=1,\n",
    ")\n",
    "\n",
    "# Save frame_id into detection to know which tracks have no detections on current frame\n",
    "frame_id = 0\n",
    "#######################################################\n",
    "\n",
    "for img, pred_df in tqdm(iter_test):\n",
    "    anno = ''\n",
    "    bboxes = np.empty((0,5), int)\n",
    "    predictions, detects = [], []\n",
    "    \n",
    "    r = model(img, size=img_size, augment=augment)\n",
    "\n",
    "    if r.pandas().xyxy[0].shape[0] == 0:\n",
    "        anno = ''\n",
    "    else:\n",
    "        for idx, row in r.pandas().xyxy[0].iterrows():\n",
    "            if row.confidence > confthre:\n",
    "                bboxes = np.append(bboxes, [[row.xmin, row.ymin, row.xmax, row.ymax, row.confidence]], axis=0)\n",
    "        \n",
    "        for bbox in bboxes:\n",
    "            detects.append(bbox)\n",
    "            width, height = int(bbox[2]-bbox[0]), int(bbox[3]-bbox[1])\n",
    "            area = width * height\n",
    "            if area >= area_thr:\n",
    "                predictions.append('{:.2f} {} {} {} {}'.format(bbox[4], int(bbox[0]), int(bbox[1]), width, height))\n",
    "                                   \n",
    "    #######################################################\n",
    "    #                      Tracking                       #\n",
    "    #######################################################\n",
    "    \n",
    "    # Update tracks using detects from current frame\n",
    "    tracked_objects = tracker.update(detections=to_norfair(detects, frame_id))\n",
    "    for tobj in tracked_objects:\n",
    "        bbox_width, bbox_height, last_detected_frame_id = tobj.last_detection.data\n",
    "        if last_detected_frame_id == frame_id:  # Skip objects that were detected on current frame\n",
    "            continue\n",
    "            \n",
    "        # Add objects that have no detections on current frame to predictions\n",
    "        xc, yc = tobj.estimate[0]\n",
    "        x_min, y_min = int(round(xc - bbox_width / 2)), int(round(yc - bbox_height / 2))\n",
    "        score = tobj.last_detection.scores[0]\n",
    "        area = bbox_width * bbox_height\n",
    "        if area >= area_thr:\n",
    "            predictions.append('{:.2f} {} {} {} {}'.format(score, x_min, y_min, bbox_width, bbox_height))\n",
    "    #######################################################\n",
    "    \n",
    "    prediction_str = ' '.join(predictions)\n",
    "    pred_df['annotations'] = prediction_str\n",
    "    env.predict(pred_df)\n",
    "\n",
    "    print('Prediction:', prediction_str)\n",
    "    frame_id += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv('submission.csv')\n",
    "sub_df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
